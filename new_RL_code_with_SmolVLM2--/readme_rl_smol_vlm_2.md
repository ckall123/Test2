# RL_code_with_SmolVLM2

A structured reinforcement learning (RL) framework for robotic tabletop object rearrangement using ROS 2, SmolVLM2 vision-language model, and Stable-Baselines3.

---

## ğŸ“¦ Project Structure

```
RL_code_with_SmolVLM2/
â”œâ”€â”€ train.py                # Training entrypoint
â”œâ”€â”€ eval.py                 # Evaluation script
â”œâ”€â”€ callbacks.py            # TensorBoard + metric logging
â”‚
â”œâ”€â”€ envs/                   # Custom Gymnasium environment
â”‚   â”œâ”€â”€ xarm6_env.py
â”‚
â”œâ”€â”€ objects/                # Simulated objects & spawning logic
â”‚   â”œâ”€â”€ sim_object.py
â”‚   â””â”€â”€ spawner.py
â”‚
â”œâ”€â”€ reward/                 # Reward functions
â”‚   â””â”€â”€ reward.py
â”‚
â”œâ”€â”€ vlm/                    # Vision-Language Model interface
â”‚   â”œâ”€â”€ sync_api.py         # VLM scoring (sync)
â”‚   â””â”€â”€ vlm_core.py         # Cached VLM interface (mockable)
â”‚
â”œâ”€â”€ models/                 # Feature extractors for SAC
â”‚   â””â”€â”€ feature_extractor.py
â”‚
â””â”€â”€ rl_utils/               # Utilities (image, gripper control)
    â”œâ”€â”€ image.py
    â””â”€â”€ gripper_control.py
```

---

## ğŸ”§ Features

### Environment (`envs/xarm6_env.py`)
- Compatible with Gymnasium API
- ROS 2 powered xArm6 joint & gripper control
- Automatic attach/detach simulation for object grasping
- VLM evaluation support (layout scoring)
- Modular reward design (geom/layout/VLM)

### VLM Integration
- Periodically (configurable `VLM_INTERVAL`) sends downsampled image to `sync_api.get_vlm_score`
- Score (0-1) reflects how well objects are arranged
- Prompt is configurable: `"æ¡Œé¢ç‰©å“æ•´é½Šæ’åˆ—ã€ç­‰é–“è·ã€é‚Šç·£å°é½Šã€‚"`

### Rewards (`reward/reward.py`)
- `geom_reward`: closeness to object, with workspace penalties
- `layout_score`: alignment + spacing among objects
- `final_reward`: weighted sum with optional VLM score boost

### Training (`train.py`)
- Uses Stable-Baselines3 SAC + custom feature extractor (SmolVLM2)
- Environment wrapped in Monitor + TimeLimit + DummyVecEnv
- TensorBoard logging (reward, gripper, actions, VLM layout scores)
- Automatic checkpointing & best model saving

### Evaluation (`eval.py`)
- Loads trained model and runs multiple episodes
- Logs and prints reward + layout + VLM statistics

### Object Simulation
- `SimObject`: track object position, link, and attachment
- `Spawner`: spawn URDF objects into Gazebo from model database

---

## ğŸ›  Execution Notes

### ROS 2 Warning
You may see:
```
Publisher already registered for provided node name...
```
This is safe and caused by node name reuse. You can ignore it if functionality is normal.

### Eval Callback Warning
```
Training and eval env are not of the same type
```
This is safe: it comes from `VecTransposeImage` used by Stable-Baselines3. If you don't explicitly use image transposition, you can suppress this.

---

## ğŸ§  Future Improvements
- Switch to `async_api.py` for parallel VLM inference
- Multi-object layout planning reward
- Full evaluation script with CSV export
- Replace mocked VLM with real API call

---

## ğŸ“‹ Summary
This codebase allows you to:
- Train an RL agent to rearrange objects on a table
- Score progress with layout/VLM-based metrics
- Evaluate agent policies with layout-aware vision models

Clean modular structure ensures that future research (e.g. new reward signals, better VLMs, more complex scenes) is easy to prototype.

