#!/usr/bin/env python3
"""
train.py â€” RL-VLM-F pipeline for xArm6 (Joint-centric)

æµç¨‹ï¼š
1) åˆå§‹åŒ– ROS2ã€ç’°å¢ƒã€Agent(SAC)ã€å›æ”¾/å½±åƒ/åå¥½è³‡æ–™ç·©è¡å€ã€VLM èˆ‡ RewardModelã€‚
2) åè¦†é€²è¡Œè¿­ä»£ï¼š
   - Rollout N æ­¥ï¼šèˆ‡ç’°å¢ƒäº’å‹•ã€è’é›† transition èˆ‡åœ–ç‰‡ï¼ˆå« episode/step ç´¢å¼•ï¼‰ã€‚
   - å¾ ImageBuffer å–æ¨£ M å°å½±åƒï¼Œå‘¼å« VLMScorer å–å¾—åå¥½æ¨™ç±¤ï¼Œç´¯ç© PreferenceDatasetã€‚
   - ç”¨åå¥½è³‡æ–™è¨“ç·´ RewardModelï¼ˆBradleyâ€“Terry pairwise lossï¼‰ã€‚
   - ç”¨ RewardModel å° replay transitions åšã€Œdiffã€æ¨¡å¼ relabelã€‚
   - å°‡ relabeled transitions å¯«å…¥ SB3 replay bufferï¼Œåƒ…åŸ·è¡Œã€Œæ¢¯åº¦æ›´æ–°ã€ï¼ˆä¸å†æ”¶é›†æ–°è³‡æ–™ï¼‰ã€‚
3) é€±æœŸæ€§å„²å­˜æ¨¡å‹ï¼ç´€éŒ„ã€‚

æ³¨æ„ï¼š
- åƒ…åœ¨æœ¬æª”æ¡ˆå»ºç«‹ ROS2 node/executorã€‚
- SAC ä½¿ç”¨ MultiInputPolicyï¼ˆobservation æ˜¯ Dict(image, state)ï¼‰ã€‚
"""

import os
import random
from typing import List

import numpy as np
import torch
from torch import optim

import rclpy
from rclpy.executors import SingleThreadedExecutor

from stable_baselines3 import SAC

# === æœ¬å°ˆæ¡ˆæ¨¡çµ„ ===
from xarm6_gym_env import XArm6Env, XArmEnvConfig
from buffers import ReplayBuffer, ImageBuffer, PreferenceDataset
from reward import RewardModel, pairwise_loss, preprocess_image
from relabel import relabel_transitions
from vlm import VLMScorer
from collision_object import CollisionObjectManager


# ========= è¶…åƒ =========
CYCLES: int = 100                 # å¤–åœˆè¿­ä»£æ•¸
ROLLOUT_STEPS: int = 3000         # æ¯å€‹ Cycle rollout æ­¥æ•¸ N
PREF_PAIRS_PER_CYCLE: int = 50    # æ¯å€‹ Cycle é€ VLM çš„é…å°æ•¸ M
REWARD_SCALE: float = 0.1         # å¯«å…¥ SB3 buffer å‰çš„ç¸®æ”¾
BT_EPOCHS: int = 1000             # RewardModel è¨“ç·´æ­¥æ•¸ï¼ˆä¸Šé™ï¼‰
BT_BATCH: int = 32                # RewardModel æ‰¹æ¬¡å¤§å°
LR_R: float = 1e-4                # RewardModel å­¸ç¿’ç‡
SAVE_DIR: str = "runs"            # è¼¸å‡ºè³‡æ–™å¤¾


# ========= å°å·¥å…· / æ¨¡çµ„ä»‹é¢ =========
def _to_torch_batch(imgs: List[np.ndarray], device: torch.device) -> torch.Tensor:
    """å°‡ numpy å½±åƒ list è½‰ç‚ºæ‰¹æ¬¡ tensorï¼ˆB,C,H,Wï¼‰ï¼Œç”¨ reward.preprocess_imageã€‚"""
    with torch.no_grad():
        tensors = [preprocess_image(img) for img in imgs]  # Tensor(C,H,W)
    return torch.stack(tensors, dim=0).to(device)          # (B,C,H,W)


def ask_vlm_preference(scorer: VLMScorer, img_a: np.ndarray, img_b: np.ndarray) -> int:
    """
    å‘¼å« VLMScorer æ¯”è¼ƒå…©å¼µåœ–çš„ç¾æ„Ÿä½ˆå±€ã€‚
    å›å‚³ï¼š0 åå¥½ Aã€1 åå¥½ Bã€-1 ç„¡æ³•åˆ†è¾¨ï¼ˆæœƒè¢«éæ¿¾ï¼‰ã€‚
    """
    rubric = (
        "- Items are aligned in straight rows or columns.\n"
        "- Empty space between items is balanced.\n"
        "- No excessive overlap or clutter.\n"
        "- Object orientations are consistent."
    )
    return scorer.compare(img_a, img_b, rubric)


def update_policy_from_replay(agent: SAC, replay_len: int, batch_size: int, logger) -> None:
    """
    åƒ…ä»¥ç¾æœ‰ replay buffer åšã€Œæ¢¯åº¦æ›´æ–°ã€ï¼Œé¿å…å†æ¬¡èˆ‡ç’°å¢ƒäº’å‹•ã€‚
    - è‹¥ SB3 æä¾› agent.train(gradient_steps, batch_size)ï¼šç›´æ¥ä½¿ç”¨ï¼ˆç„¡æ–° rolloutsï¼‰ã€‚
    - å¦å‰‡é€€è€Œæ±‚å…¶æ¬¡ï¼šæŠŠ gradient_steps è¨­ç‚ºæ›´æ–°æ¬¡æ•¸ï¼Œèª¿ç”¨ learn(total_timesteps=1)ã€‚
    """
    updates = max(replay_len // batch_size, 1)

    # æ–¹æ¡ˆ Aï¼šç›´æ¥å‘¼å« OffPolicyAlgorithm.trainï¼ˆä¸æ”¶é›†æ–°è³‡æ–™ï¼‰
    if hasattr(agent, "train"):
        agent.train(gradient_steps=updates, batch_size=batch_size)
        logger.info(f"âœ… SAC Policy updated with {updates} gradient steps (no new rollouts)")
        return

    # æ–¹æ¡ˆ Bï¼šç‰ˆæœ¬å—é™æ™‚çš„æŠ˜è¡·ï¼ˆåƒ… 1 æ­¥ learnï¼Œä¸»è¦åƒ replay çš„å¤§é‡ gradient_stepsï¼‰
    old_gs = getattr(agent, "gradient_steps", None)
    agent.gradient_steps = updates
    agent.learn(total_timesteps=1, reset_num_timesteps=False)
    if old_gs is not None:
        agent.gradient_steps = old_gs
    logger.info(f"âœ… SAC Policy updated via 1-step learn with {updates} gradient steps")


def main():
    # ===== 1) ROS2 & è£ç½® =====
    rclpy.init()
    node = rclpy.create_node("xarm6_train")
    executor = SingleThreadedExecutor()
    executor.add_node(node)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    node.get_logger().info(f"ğŸš€ Device: {device}")

    CollisionObjectManager.default_setup(node, executor)

    try:
        # ===== 2) Env / Agent / Buffers / Models =====
        env_cfg = XArmEnvConfig()
        env = XArm6Env(node, executor, env_cfg)

        agent = SAC(
            policy="MultiInputPolicy",
            env=env,
            verbose=1,
            device=device,
            learning_rate=3e-4,
            buffer_size=100_000,
            train_freq=(1, "step"),
            gradient_steps=1,          # å¯¦éš›æ›´æ–°æ­¥æ•¸åœ¨ update_policy_from_replay å‹•æ…‹æ§åˆ¶
            batch_size=32,
            ent_coef="auto",
        )

        replay = ReplayBuffer()        # è‡ªæœ‰ replayï¼ˆåƒ…ä¾›é‡æ¨™ç”¨ï¼‰
        images = ImageBuffer()         # å½±åƒç·©è¡
        prefs = PreferenceDataset()    # åå¥½è³‡æ–™é›†

        vlm = VLMScorer(model="llama3.2-vision:11b")
        rpsi = RewardModel().to(device)
        optimizer_r = optim.Adam(rpsi.parameters(), lr=LR_R)

        global_step = 0
        episode_id = 0

        # ===== 3) Training Cycles =====
        for cycle in range(CYCLES):
            node.get_logger().info(f"\n=== Cycle {cycle} ===")

            # --- 3.1 Rollout ---
            obs, info = env.reset()
            step_in_ep = 0
            for _ in range(ROLLOUT_STEPS):
                action, _ = agent.predict(obs, deterministic=False)
                next_obs, _, terminated, truncated, next_info = env.step(action)
                done = bool(terminated or truncated)

                transition = {
                    "obs": obs,
                    "next_obs": next_obs,
                    "action": action,
                    "reward": 0.0,  # ä½”ä½ï¼Œç¨å¾Œé‡æ¨™
                    "done": done,
                    "image": info["image"],
                    "next_image": next_info["next_image"],
                }
                replay.add(transition)
                images.add(episode_id, step_in_ep, next_info["next_image"])

                global_step += 1
                step_in_ep += 1
                if done:
                    episode_id += 1
                    obs, info = env.reset()
                    step_in_ep = 0
                else:
                    obs, info = next_obs, next_info

            # --- 3.2 åå¥½ï¼šæŠ½ M å°å½±åƒå• VLM ---
            pairs = images.sample_pairs(PREF_PAIRS_PER_CYCLE)
            accepted = 0
            for imgA, imgB in pairs:
                y = ask_vlm_preference(vlm, imgA, imgB)
                if y in (0, 1):
                    prefs.add(imgA, imgB, y)
                    accepted += 1
            rej_rate = prefs.get_reject_rate()
            node.get_logger().info(f"ğŸ§® åå¥½é…å°ï¼šå–æ¨£ {len(pairs)} å°ï¼Œæ¥å— {accepted}ï¼Œæ‹’çµ•ç‡ {rej_rate:.2%}")

            # --- 3.3 è¨“ç·´ RewardModelï¼ˆBradleyâ€“Terryï¼‰---
            rpsi.train()
            for _ in range(min(BT_EPOCHS, len(prefs))):
                batch = random.sample(prefs.get_all(), k=min(BT_BATCH, len(prefs)))
                imgs0, imgs1, labels = zip(*batch)

                x0 = _to_torch_batch(list(imgs0), device)
                x1 = _to_torch_batch(list(imgs1), device)
                y = torch.tensor(labels, dtype=torch.float32, device=device)

                r0 = rpsi(x0)               # (B,)
                r1 = rpsi(x1)               # (B,)

                # === ä¿®æ­£é» 1ï¼šæ¨™ç±¤æ–¹å‘å°é½Š BT èªæ„ï¼ˆ0=A, 1=B â†’ 1 ä»£è¡¨ A>Bï¼‰ ===
                y_bt = 1.0 - y              # 0(A)â†’1, 1(B)â†’0
                loss = pairwise_loss(r0, r1, y_bt)

                optimizer_r.zero_grad()
                loss.backward()
                optimizer_r.step()
            rpsi.eval()
            node.get_logger().info("ğŸ¯ RewardModel æ›´æ–°å®Œæˆ")

            # --- 3.4 ç”¨ RewardModel é‡æ¨™ï¼ˆdiff æ¨¡å¼ï¼‰---
            relabel_transitions(
                replay=replay.data,
                model=rpsi,
                mode="diff",
                reward_range=(-1, 1),
                device=device,
            )
            node.get_logger().info("ğŸ” Replay rewards å·² relabel (diff)")

            # --- 3.5 å¯«å…¥ SB3 ReplayBuffer & åƒ…åšæ¢¯åº¦æ›´æ–° ---
            for t in replay.data:
                agent.replay_buffer.add(
                    obs=t["obs"],
                    next_obs=t["next_obs"],
                    action=t["action"],
                    reward=float(t["reward"]) * REWARD_SCALE,
                    done=t["done"],
                    infos={},
                )

            # === ä¿®æ­£é» 2ï¼šåªåšæ¢¯åº¦æ›´æ–°ï¼Œä¸å†è§¸ç™¼æ–° rollouts ===
            update_policy_from_replay(
                agent=agent,
                replay_len=len(replay),
                batch_size=agent.batch_size,
                logger=node.get_logger(),
            )

            # æœ¬è¼ªé‡æ¨™è³‡æ–™å·²ç”¨å®Œï¼Œæ¸…é™¤è‡ªæœ‰ replayï¼ˆSB3 å…§éƒ¨ buffer ä¿ç•™ï¼‰
            replay.clear()

        # ===== 4) æ”¶å°¾èˆ‡ä¿å­˜ =====
        os.makedirs(SAVE_DIR, exist_ok=True)
        agent.save(os.path.join(SAVE_DIR, "sac_xarm6_policy"))
        torch.save(rpsi.state_dict(), os.path.join(SAVE_DIR, "reward_model.pt"))
        node.get_logger().info("ğŸ’¾ å·²ä¿å­˜ policy èˆ‡ reward model")

    finally:
        try:
            env.close()
        except Exception:
            pass
        node.destroy_node()
        rclpy.shutdown()
        print("ğŸ‰ è¨“ç·´å®Œæˆ")


if __name__ == "__main__":
    main()
