#!/usr/bin/env python3
"""
train.py â€” RL-VLM-F pipeline for xArm6 (Joint-centric)

æµç¨‹ï¼š
1) åˆå§‹åŒ– ROS2ã€ç’°å¢ƒã€Agent(SAC)ã€å›æ”¾/å½±åƒ/åå¥½è³‡æ–™ç·©è¡å€ã€VLM èˆ‡ RewardModelã€‚
2) åè¦†é€²è¡Œè¿­ä»£ï¼š
   - Rollout N æ­¥ï¼šèˆ‡ç’°å¢ƒäº’å‹•ã€è’é›† transition èˆ‡åœ–ç‰‡ï¼ˆå« episode/step ç´¢å¼•ï¼‰ã€‚
   - å¾ ImageBuffer å–æ¨£ M å°å½±åƒï¼Œå‘¼å« VLMScorer å–å¾—åå¥½æ¨™ç±¤ï¼Œç´¯ç© PreferenceDatasetã€‚
   - ç”¨åå¥½è³‡æ–™è¨“ç·´ RewardModelï¼ˆBradleyâ€“Terry pairwise lossï¼‰ã€‚
   - ç”¨ RewardModel å° replay transitions åšã€Œdiffã€æ¨¡å¼ relabelã€‚
   - å°‡ relabeled transitions å¯«å…¥ Stable-Baselines3 çš„ replay bufferï¼Œå‘¼å« agent.learn()ã€‚
3) é€±æœŸæ€§å„²å­˜æ¨¡å‹ï¼ç´€éŒ„ã€‚

æ³¨æ„ï¼š
- åªåœ¨æœ¬æª”æ¡ˆå»ºç«‹ ROS2 node/executorã€‚
- SAC ä½¿ç”¨ MultiInputPolicyï¼ˆå› ç‚º observation æ˜¯ Dict(image,state)ï¼‰ã€‚
"""

import os
import random
from typing import Dict, Any, Tuple, List

import numpy as np
import torch
from torch import optim

import rclpy
from rclpy.executors import SingleThreadedExecutor

from stable_baselines3 import SAC

# === æœ¬å°ˆæ¡ˆæ¨¡çµ„ ===
from xarm6_gym_env import XArm6Env, XArmEnvConfig
from buffers import ReplayBuffer, ImageBuffer, PreferenceDataset
from reward import RewardModel, pairwise_loss, preprocess_image
from relabel import relabel_transitions
from vlm import VLMScorer
from collision_object import CollisionObjectManager


# ========= è¶…åƒï¼ˆå¯ä¾éœ€æ±‚èª¿æ•´ï¼‰ =========
CYCLES: int = 100                 # è¨“ç·´å¤–åœˆè¿­ä»£æ•¸
ROLLOUT_STEPS: int = 3000         # æ¯å€‹ Cycle rollout æ­¥æ•¸ N
PREF_PAIRS_PER_CYCLE: int = 50    # æ¯å€‹ Cycle é€ VLM çš„é…å°æ•¸ M
REWARD_SCALE: float = 0.1         # å¯¦éš›å¯«å…¥ SAC buffer å‰çš„ç¸®æ”¾
BT_EPOCHS: int = 1000             # RewardModel è¨“ç·´æ­¥æ•¸
BT_BATCH: int = 32                # RewardModel æ‰¹æ¬¡å¤§å°
LR_R: float = 1e-4                # RewardModel å­¸ç¿’ç‡
SAVE_DIR: str = "runs"            # ç´€éŒ„è¼¸å‡ºè³‡æ–™å¤¾


# ========= å°å·¥å…· =========
def _to_torch_batch(imgs: List[np.ndarray], device: torch.device) -> torch.Tensor:
    """å°‡ numpy å½±åƒé™£åˆ— list è½‰ç‚ºæ‰¹æ¬¡ tensorï¼ˆB,C,H,Wï¼‰ï¼Œç”¨ reward.preprocess_imageã€‚"""
    with torch.no_grad():
        tensors = [preprocess_image(img) for img in imgs]  # å›å‚³å·²æ­£è¦åŒ–çš„ Tensor(C,H,W)
    x = torch.stack(tensors, dim=0).to(device)             # (B,C,H,W)
    return x


def ask_vlm_preference(scorer: VLMScorer, img_a: np.ndarray, img_b: np.ndarray) -> int:
    """
    å‘¼å« VLMScorer æ¯”è¼ƒå…©å¼µåœ–çš„æ•´é½Šåº¦ã€‚
    å›å‚³ï¼š0 åå¥½ Aã€1 åå¥½ Bã€-1 ç„¡æ³•åˆ†è¾¨ï¼ˆæœƒè¢«éæ¿¾ï¼‰ã€‚
    """
    rubric = (
        "- Items are aligned in straight rows or columns.\n"
        "- Empty space between items is balanced.\n"
        "- No excessive overlap or clutter.\n"
        "- Object orientations are consistent."
    )
    return scorer.compare(img_a, img_b, rubric)


def main():
    # ===== 1) ROS2 & è£ç½® =====
    rclpy.init()
    node = rclpy.create_node("xarm6_train")
    executor = SingleThreadedExecutor()
    executor.add_node(node)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    node.get_logger().info(f"ğŸš€ Device: {device}")

    CollisionObjectManager.default_setup(node, executor)

    try:
        # ===== 2) Env / Agent / Buffers / Models =====
        env_cfg = XArmEnvConfig()
        env = XArm6Env(node, executor, env_cfg)

        # MultiInputPolicyï¼ˆobservation ç‚º Dict("image","state")ï¼‰
        agent = SAC(
            policy="MultiInputPolicy",
            env=env,
            verbose=1,
            device=device,
            learning_rate=3e-4,
            buffer_size=100_000,
            train_freq=(1, "step"),
            gradient_steps=1,
            batch_size=32,
            ent_coef="auto"
        )

        # ç·©è¡å€
        replay = ReplayBuffer()         # è‡ªæœ‰ replayï¼ˆç”¨æ–¼é‡æ¨™æ³¨ï¼‰
        images = ImageBuffer()          # ä¾ episode/step è’é›†å½±åƒ
        prefs = PreferenceDataset()     # (imgA, imgB, label)

        # VLM & Reward
        vlm = VLMScorer(model="llama3.2-vision:11b")
        rpsi = RewardModel().to(device)
        optimizer_r = optim.Adam(rpsi.parameters(), lr=LR_R)

        global_step = 0
        episode_id = 0

        # ===== 3) Training Cycles =====
        for cycle in range(CYCLES):
            node.get_logger().info(f"\n=== Cycle {cycle} ===")

            # --- 3.1 Rollout ---
            obs, info = env.reset()
            step_in_ep = 0
            for _ in range(ROLLOUT_STEPS):
                # å¾ SAC policy å–å‹•ä½œ
                action, _ = agent.predict(obs, deterministic=False)

                next_obs, _, terminated, truncated, next_info = env.step(action)
                done = bool(terminated or truncated)

                # è‡ªæœ‰ replay transitionï¼ˆç”¨æ–¼å¾ŒçºŒ relabelï¼‰
                transition = {
                    "obs": obs,                         # Dict obsï¼ˆSB3 éœ€è¦ï¼‰
                    "next_obs": next_obs,               # Dict next_obs
                    "action": action,
                    "reward": 0.0,                      # å…ˆä½”ä½ï¼Œç¨å¾Œé‡æ¨™æ³¨
                    "done": done,
                    "image": info["image"],             # å‰å½±åƒ
                    "next_image": next_info["next_image"]
                }
                replay.add(transition)

                # å½±åƒç·©è¡ï¼ˆfor åå¥½é…å°ï¼‰
                images.add(episode_id, step_in_ep, next_info["next_image"])

                # episode æ§åˆ¶
                global_step += 1
                step_in_ep += 1
                if done:
                    episode_id += 1
                    obs, info = env.reset()
                    step_in_ep = 0
                else:
                    obs, info = next_obs, next_info

            # --- 3.2 åå¥½ï¼šæŠ½ M å°å½±åƒå• VLM ---
            pairs = images.sample_pairs(PREF_PAIRS_PER_CYCLE)
            accepted = 0
            for imgA, imgB in pairs:
                y = ask_vlm_preference(vlm, imgA, imgB)
                if y in (0, 1):
                    prefs.add(imgA, imgB, y)
                    accepted += 1
            rej_rate = prefs.get_reject_rate()
            node.get_logger().info(f"ğŸ§® åå¥½é…å°ï¼šå–æ¨£ {len(pairs)} å°ï¼Œæ¥å— {accepted}ï¼Œæ‹’çµ•ç‡ {rej_rate:.2%}")

            # --- 3.3 è¨“ç·´ RewardModelï¼ˆBradleyâ€“Terryï¼‰---
            rpsi.train()
            for _ in range(min(BT_EPOCHS, len(prefs))):
                batch = random.sample(prefs.get_all(), k=min(BT_BATCH, len(prefs)))
                imgs0, imgs1, labels = zip(*batch)

                x0 = _to_torch_batch(list(imgs0), device)
                x1 = _to_torch_batch(list(imgs1), device)
                y = torch.tensor(labels, dtype=torch.float32, device=device)

                r0 = rpsi(x0)  # (B,)
                r1 = rpsi(x1)  # (B,)
                loss = pairwise_loss(r0, r1, y)

                optimizer_r.zero_grad()
                loss.backward()
                optimizer_r.step()
            rpsi.eval()
            node.get_logger().info("ğŸ¯ RewardModel æ›´æ–°å®Œæˆ")

            # --- 3.4 ç”¨ RewardModel é‡æ¨™æ³¨ï¼ˆdiff æ¨¡å¼ï¼‰---
            relabel_transitions(
                replay=replay.data,
                model=rpsi,
                mode="diff",
                reward_range=(-1, 1),
                device=device
            )
            node.get_logger().info("ğŸ” Replay rewards å·² relabel (diff)")

            # --- 3.5 å¯«å…¥ SB3 ReplayBuffer ä¸¦å­¸ç¿’ ---
            # å°‡ relabeled transitions å¯«å…¥ agent.replay_buffer
            for t in replay.data:
                # SB3 DictReplayBuffer æ”¯æ´ dict obs
                agent.replay_buffer.add(
                    obs=t["obs"],
                    next_obs=t["next_obs"],
                    action=t["action"],
                    reward=float(t["reward"]) * REWARD_SCALE,
                    done=t["done"],
                    infos={}
                )

            # ä½¿ç”¨ç›®å‰ buffer åšå­¸ç¿’ï¼ˆä¸é‡ç½® timestepï¼‰
            agent.learn(total_timesteps=len(replay), log_interval=10, reset_num_timesteps=False)
            node.get_logger().info(f"âœ… SAC Policy Updated (Cycle {cycle})")

            # ï¼ˆé¸æ“‡æ€§ï¼‰æ¸…ç†/ä¿ç•™è³‡æ–™
            replay.clear()   # æœ¬è¼ªé‡æ¨™æ³¨å·²ç”¨å®Œï¼Œæ¸…æ‰
            # images / prefs å¯ç´¯ç©ï¼Œäº¦å¯è¦–éœ€æ±‚æ¸…ç†ï¼š
            # images.clear(); prefs.clear()

        # ===== 4) æ”¶å°¾èˆ‡ä¿å­˜ =====
        os.makedirs(SAVE_DIR, exist_ok=True)
        agent.save(os.path.join(SAVE_DIR, "sac_xarm6_policy"))
        torch.save(rpsi.state_dict(), os.path.join(SAVE_DIR, "reward_model.pt"))
        node.get_logger().info("ğŸ’¾ å·²ä¿å­˜ policy èˆ‡ reward model")

    finally:
        # é—œé–‰ env / ROS
        try:
            env.close()
        except Exception:
            pass
        node.destroy_node()
        rclpy.shutdown()
        print("ğŸ‰ è¨“ç·´å®Œæˆ")
        

if __name__ == "__main__":
    main()
