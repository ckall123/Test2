#!/usr/bin/env python3
import rclpy
from rclpy.executors import SingleThreadedExecutor
import torch
import random
from reward import RewardModel, compute_bt_loss, make_transforms, relabel_transitions
from xarm6_gym_env import XArm6Env, XArmEnvConfig
from utils import sample_image_pairs, ask_vlm_preference
from stable_baselines3 import SAC

# âœ… åˆå§‹åŒ– ROS2
rclpy.init()
node = rclpy.create_node('xarm6_train')
executor = SingleThreadedExecutor()
executor.add_node(node)

# âœ… åˆå§‹åŒ–ç’°å¢ƒèˆ‡æ¨¡å‹
env = XArm6Env(node, executor, XArmEnvConfig())
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
rpsi = RewardModel().to(device)

# âœ… åˆå§‹åŒ– SAC Agentï¼ˆä½¿ç”¨ Stable-Baselines3ï¼‰
# è‹¥ obs æ˜¯ Dict("state", "image") çµæ§‹ï¼Œå¯æ”¹ MultiInputPolicy
agent = SAC("MlpPolicy", env, verbose=1, device=device, learning_rate=3e-4, buffer_size=100000)

# âœ… åˆå§‹åŒ– buffer
replay_buffer = []  # B
image_buffer = []   # I
pref_dataset = []   # D

# âœ… Hyperparams
N = 3000         # rollout æ­¥æ•¸
M = 50           # æ¯è¼ªåå¥½ pair æ•¸
reward_scale = 0.1

for cycle in range(100):
    print(f"\n=== Cycle {cycle} é–‹å§‹ ===")

    # 1ï¸âƒ£ Rolloutï¼šäº’å‹• N æ­¥
    obs, info = env.reset()
    for _ in range(N):
        action, _ = agent.predict(obs, deterministic=False)
        next_obs, _, done, truncated, next_info = env.step(action)

        transition = {
            'state': obs['state'],
            'action': action,
            'next_state': next_obs['state'],
            'image': info['image'],
            'next_image': info['next_image'],
            'reward': 0.0  # placeholderï¼Œä¹‹å¾Œæœƒ relabel
        }
        replay_buffer.append(transition)
        image_buffer.append(info['next_image'])

        if done or truncated:
            obs, info = env.reset()
        else:
            obs, info = next_obs, next_info

    # 2ï¸âƒ£ Preferenceï¼šæŠ½ M å°å• VLM
    pairs = sample_image_pairs(image_buffer, M)
    for imgA, imgB in pairs:
        y = ask_vlm_preference(imgA, imgB)
        if y in [0, 1]:
            pref_dataset.append((imgA, imgB, y))

    # 3ï¸âƒ£ Train rÏˆ (Reward Model)
    optimizer = torch.optim.Adam(rpsi.parameters(), lr=1e-4)
    transform = make_transforms()
    rpsi.train()

    for _ in range(1000):
        batch = random.sample(pref_dataset, min(32, len(pref_dataset)))
        imgs0, imgs1, labels = zip(*batch)
        x0 = torch.stack([transform(img) for img in imgs0]).to(device)
        x1 = torch.stack([transform(img) for img in imgs1]).to(device)
        y = torch.tensor(labels, dtype=torch.float32).to(device)

        r0 = rpsi(x0)
        r1 = rpsi(x1)
        loss = compute_bt_loss(r0, r1, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # 4ï¸âƒ£ Relabel rewards
    relabel_transitions(replay_buffer, rpsi, device, diff_mode=True)

    # 5ï¸âƒ£ Policy Update
    # æŠŠ relabeled reward å¯«å…¥ SB3 replay buffer ä¸¦å­¸ç¿’
    for t in replay_buffer:
        agent.replay_buffer.add(
            t['state'], t['next_state'],
            t['action'], t['reward'] * reward_scale,
            done=False
        )

    agent.learn(total_timesteps=len(replay_buffer), log_interval=10)
    print(f"âœ… SAC Policy Updated (Cycle {cycle})")

# âœ… æ”¶å°¾
agent.save("sac_xarm6_policy")
node.destroy_node()
rclpy.shutdown()
print("ğŸ¯ è¨“ç·´å®Œæˆï¼Œæ¨¡å‹å·²å„²å­˜ç‚º sac_xarm6_policy.zip")
