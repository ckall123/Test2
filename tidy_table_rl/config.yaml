# 固定種子與裝置設定
seed: 42                     # 隨機種子，確保實驗可重現
device: "cuda:0"             # 訓練使用裝置，例如 "cuda:0" 或 "cpu"

# 環境參數（xArm 桌面與相機設定）
env:
  x_range: [-1.05, 0.45]     # 桌面允許生成物件的 X 範圍（米）
  y_range: [-1.20, -0.40]    # 桌面允許生成物件的 Y 範圍（米）
  z_height: 1.015            # 桌面高度 Z 值（米）
  arm_exclude_x: [-0.45, -0.11]  # 手臂活動區排除的 X 區域
  arm_exclude_y: [-0.67, -0.26]  # 手臂活動區排除的 Y 區域
  camera_topic: "/top_camera/image_raw"  # 相機影像 ROS topic 名稱
  resolution: [480, 640]     # 相機輸入影像解析度（高度, 寬度）
  roi: null                  # 選用的視野裁切區域（預設 null 為不裁切）

# 訓練循環控制
cycle:
  N_rollout: 3000            # 每輪與環境互動的步數
  M_pairs: 50                # 每輪要建立的偏好圖對數
  rpsi_train_steps: 10000    # 每輪訓練 reward model 的步數
  sac_update_steps: 10000    # 每輪 SAC 訓練步數
  N_envs: 1                  # 使用的環境實例數（未來支援多環境時可用）

# SAC 強化學習超參數
sac:
  lr_actor: 3e-4             # actor 網路學習率
  lr_critic: 3e-4            # critic 網路學習率
  gamma: 0.99                # 折扣因子
  auto_entropy: true         # 是否自動調整 entropy 溫度
  grad_clip: 10.0            # 梯度裁切上限

# Reward 模型與 relabel 設定
reward:
  mode: diff                 # 差分型 reward（"diff"）或絕對分數（"absolute"）
  scale: zscore              # 獎勵縮放方式：zscore 或 none
  tau: 1.0                   # 用於 reward smoothing 的參數（若使用）
  clip: [-5.0, 5.0]          # reward 裁切上下限
  rpsi:
    input_size: [224, 224]   # reward model 輸入影像尺寸
    transforms_profile: imagenet   # 使用的影像轉換配置：imagenet / custom
    # 若使用 custom，請打開以下欄位指定 mean/std：
    # mean: [0.485, 0.456, 0.406]
    # std: [0.229, 0.224, 0.225]

# 視覺語言模型（VLM）偏好教師設定
vlm:
  model: "llama3.2-vision:11b"  # 使用的 VLM 模型名稱
  rubric: |                   # 偏好評分標準（會寫進 prompt）
    - Items are aligned in straight rows or columns.
    - Empty space between items is balanced.
    - No excessive overlap or clutter.
    - Object orientations are consistent.

# 訓練過程紀錄與可視化設定
logging:
  log_dir: "./logs"           # 日誌與模型儲存目錄
  ckpt_interval: 1            # 每幾輪儲存一次 checkpoint
  save_interval: 1            # 每幾輪儲存一次完整模型
  visualize: true             # 是否啟用可視化（如 tensorboard、畫圖）
