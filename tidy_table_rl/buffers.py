#!/usr/bin/env python3
"""
buffers.py

å®šç¾© RL-VLM-F æ‰€éœ€çš„ä¸‰å€‹ç·©è¡å€ï¼š
1. ImageBuffer (I): å„²å­˜é«˜å“è³ªå½±åƒï¼Œä¾› VLM æŠ½æ¨£ã€‚
2. PreferenceDataset (D): å„²å­˜ VLM æ¨™è¨»çš„åå¥½å°ã€‚
3. ReplayBuffer (B): å„²å­˜ (s, a, s', imgs...)ï¼Œä¸¦æä¾› relabel åŠŸèƒ½ã€‚
"""

import numpy as np
import random
from collections import deque
from typing import Tuple, List, Dict, Any, Callable

# ç¢ºä¿å½±åƒå„²å­˜ç‚º uint8 ä»¥ç¯€çœè¨˜æ†¶é«”
Image = np.ndarray  # Type hint for HWC, uint8 image

# =====================================
# ğŸ“¸ å½±åƒæ±  (I)
# =====================================

class ImageBuffer:
    """ å½±åƒç·©è¡å€ (I)ï¼Œå„²å­˜é«˜å“è³ªå½±åƒä¾› VLM æ¡æ¨£ """
    def __init__(self, max_size: int):
        self.max_size = max_size
        self._buffer = deque(maxlen=max_size)

    def __len__(self) -> int:
        return len(self._buffer)

    def add(self, image: Image):
        """ æ–°å¢ä¸€å¼µé«˜å“è³ªå½±åƒ (uint8) """
        if image.dtype != np.uint8:
            image = np.clip(image, 0, 255).astype(np.uint8)
        self._buffer.append(image)

    def sample_pair(self) -> Tuple[Image, Image] | None:
        """ éš¨æ©ŸæŠ½å–ä¸€å°å½±åƒ (sigma_0, sigma_1) """
        if len(self._buffer) < 2:
            return None
        
        # ç¢ºä¿æŠ½åˆ°å…©å¼µä¸åŒçš„å½±åƒ
        idx1, idx2 = random.sample(range(len(self._buffer)), 2)
        return self._buffer[idx1], self._buffer[idx2]

# =====================================
# ğŸ‘ åå¥½è³‡æ–™é›† (D)
# =====================================

@np.record
class Preference:
    """ å„²å­˜ä¸€å€‹åå¥½æ¨£æœ¬ """
    image_0: Image
    image_1: Image
    label: int  # 0 (åå¥½ 0), 1 (åå¥½ 1)

class PreferenceDataset:
    """ åå¥½è³‡æ–™é›† (D)ï¼Œå„²å­˜ VLM çš„æ¨™è¨»çµæœ """
    def __init__(self, max_size: int):
        self.max_size = max_size
        self._buffer = deque(maxlen=max_size)

    def __len__(self) -> int:
        return len(self._buffer)

    def add(self, image_0: Image, image_1: Image, label: int):
        """ æ–°å¢ä¸€ç­†åå¥½ (y=0 æˆ– y=1) """
        if label not in (0, 1):
            raise ValueError("æ¨™ç±¤ 'label' å¿…é ˆæ˜¯ 0 æˆ– 1")
        
        # ç¢ºä¿ uint8
        if image_0.dtype != np.uint8:
            image_0 = np.clip(image_0, 0, 255).astype(np.uint8)
        if image_1.dtype != np.uint8:
            image_1 = np.clip(image_1, 0, 255).astype(np.uint8)
            
        self._buffer.append(Preference(image_0, image_1, label))

    def sample(self, batch_size: int) -> List[Preference]:
        """ éš¨æ©ŸæŠ½æ¨£ä¸€å€‹ batch çš„åå¥½ """
        batch_size = min(batch_size, len(self._buffer))
        return random.sample(self._buffer, batch_size)

# =====================================
# ğŸ”„ é‡æ’­ç·©è¡å€ (B)
# =====================================

class ReplayBuffer:
    """ 
    é‡æ’­ç·©è¡å€ (B)ï¼Œå„²å­˜ (s, a, s', img_t, img_{t+1}, done)ã€‚
    æ ¸å¿ƒåŠŸèƒ½: relabel()ï¼Œä½¿ç”¨ r_psi è¨ˆç®—å·®åˆ†çå‹µã€‚
    """
    def __init__(self, state_dim: int, action_dim: int, max_size: int):
        self.max_size = max_size
        self.ptr = 0
        self.size = 0

        # ç‹€æ…‹ (s, s')
        self.state = np.zeros((max_size, state_dim), dtype=np.float32)
        self.next_state = np.zeros((max_size, state_dim), dtype=np.float32)
        # å‹•ä½œ (a)
        self.action = np.zeros((max_size, action_dim), dtype=np.float32)
        # å½±åƒ (img_t, img_{t+1}) - ä½¿ç”¨ object array å„²å­˜
        self.image = np.empty((max_size, 1), dtype=object)
        self.next_image = np.empty((max_size, 1), dtype=object)
        # çå‹µ (r) - ç­‰å¾… relabel
        self.reward = np.zeros((max_size, 1), dtype=np.float32)
        # çµæŸ (done)
        self.done = np.zeros((max_size, 1), dtype=np.float32)

    def __len__(self) -> int:
        return self.size

    def add(self, 
            state: np.ndarray, 
            action: np.ndarray, 
            next_state: np.ndarray, 
            image: Image,         # image_t
            next_image: Image,    # image_{t+1}
            done: bool):
        """
        æ–°å¢ä¸€ç­† transitionã€‚
        æ³¨æ„: reward é è¨­ç‚º 0ï¼Œç­‰å¾… relabel()ã€‚
        """
        self.state[self.ptr] = state
        self.action[self.ptr] = action
        self.next_state[self.ptr] = next_state
        self.image[self.ptr, 0] = image
        self.next_image[self.ptr, 0] = next_image
        self.reward[self.ptr] = 0.0  # é è¨­ç‚º 0
        self.done[self.ptr] = float(done)

        self.ptr = (self.ptr + 1) % self.max_size
        self.size = min(self.size + 1, self.max_size)

    def relabel(self, reward_model_fn: Callable[[Image], float]):
        """
        ä½¿ç”¨æ–°çš„ r_psi æ¨¡å‹é‡æ–°æ¨™è¨»**æ‰€æœ‰**çå‹µã€‚
        r_t = r_psi(img_{t+1}) - r_psi(img_t)
        """
        if self.size == 0:
            return

        print(f"[Relabel] æ­£åœ¨ç‚º {self.size} ç­† transition é‡æ–°è¨ˆç®—çå‹µ...")
        
        # 1. æ‰¹æ¬¡è¨ˆç®—æ‰€æœ‰ img_t å’Œ img_{t+1} çš„åˆ†æ•¸
        # (é€™è£¡å‡è¨­ reward_model_fn å¯ä»¥æ¥å—æ‰¹æ¬¡è¼¸å…¥ï¼Œå¦‚æœä¸è¡Œï¼Œéœ€æ”¹ç‚º for è¿´åœˆ)
        try:
            # å–å‡ºæ‰€æœ‰å½±åƒ
            imgs_t = np.stack(self.image[:self.size, 0])      # (size, H, W, 3)
            imgs_t_plus_1 = np.stack(self.next_image[:self.size, 0]) # (size, H, W, 3)

            # æ‰¹æ¬¡æ‰“åˆ†
            scores_t = reward_model_fn(imgs_t)          # (size,)
            scores_t_plus_1 = reward_model_fn(imgs_t_plus_1)  # (size,)

        except ValueError: 
            # å¦‚æœå †ç–Šå¤±æ•— (ä¾‹å¦‚å½±åƒå¤§å°ä¸ä¸€) æˆ–æ¨¡å‹ä¸æ”¯æ´æ‰¹æ¬¡ï¼Œé€€å› for è¿´åœˆ
            print("[Relabel] æ‰¹æ¬¡è™•ç†å¤±æ•—ï¼Œé€€å› for è¿´åœˆ (è¼ƒæ…¢)")
            scores_t = np.array([reward_model_fn(img) for img in self.image[:self.size, 0]])
            scores_t_plus_1 = np.array([reward_model_fn(img) for img in self.next_image[:self.size, 0]])
        
        # 2. è¨ˆç®—å·®åˆ†çå‹µ
        diff_rewards = scores_t_plus_1 - scores_t
        
        # 3. è¦†å¯«ç·©è¡å€ä¸­çš„çå‹µ
        self.reward[:self.size] = diff_rewards.reshape(-1, 1)
        
        print(f"[Relabel] å®Œæˆã€‚çå‹µå¹³å‡å€¼: {np.mean(diff_rewards):.4f}")

    def sample(self, batch_size: int) -> Dict[str, np.ndarray]:
        """ 
        ç‚º SAC (Policy) æŠ½æ¨£ä¸€å€‹ batch çš„ (s, a, r, s', done)ã€‚
        æ³¨æ„: é€™è£¡å›å‚³çš„ r æ˜¯å·²ç¶“è¢« relabel éçš„ã€‚
        """
        idxs = np.random.randint(0, self.size, size=batch_size)

        return {
            "state": self.state[idxs],
            "action": self.action[idxs],
            "reward": self.reward[idxs],  # å›å‚³å·² relabel çš„çå‹µ
            "next_state": self.next_state[idxs],
            "done": self.done[idxs]
            # æ³¨æ„: SAC ä¸éœ€è¦å½±åƒï¼Œæ‰€ä»¥é€™è£¡ä¸å›å‚³
        }