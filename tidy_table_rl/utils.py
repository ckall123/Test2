# utils.py
# é€šç”¨è¼”åŠ©å·¥å…·ï¼ˆèˆ‡ ROS/MoveIt ç„¡é—œï¼‰
# - ç’°å¢ƒè¨­å®š EnvConfig
# - å½±åƒè™•ç† preprocess_image
# - å‹•ä½œæ­·å² ActionHistory
# - å‹•ä½œæ‡²ç½° compute_action_cost
# - VLM æ§åˆ¶åŠ åˆ† VLMThrottle
# - è·é›¢åˆ†æ®µçå‹µ distance_sparse_bonus

from dataclasses import dataclass
from collections import deque
from typing import Deque, Optional, Tuple, Sequence

import numpy as np
import cv2


# =====================================
# ğŸ§© ç’°å¢ƒè¨­å®š Config
# =====================================
@dataclass
class EnvConfig:
    # åŸºæœ¬åƒæ•¸
    image_size: Tuple[int, int] = (96, 96)
    max_steps: int = 400
    action_scale: float = 0.08

    # å¹¾ä½•èƒ½é‡æ¬Šé‡
    w_pca: float = 1.0
    w_spacing: float = 1.0
    w_yaw: float = 0.5
    w_overlap: float = 2.0
    w_out: float = 1.5
    overlap_lambda: float = 1.1
    edge_margin: float = 0.03
    E_success: float = 0.10

    # ç›®æ¨™è·é›¢æˆåŠŸåˆ¤å®šï¼ˆDone æ¢ä»¶å¯ç”¨ï¼‰
    success_dist: float = 0.01

    # æ¥è§¸ä¸€æ¬¡æ€§åŠ åˆ†
    contact_bonus: float = 0.2

    # æ‡²ç½°è¨­è¨ˆ
    prox_weight: float = 0.5
    action_hist_len: int = 10
    action_cost_coef: float = 0.01
    collision_penalty: float = 1.0

    # VLM åŠ åˆ†æ§åˆ¶
    vlm_interval: int = 8
    vlm_deltaE_thresh: float = 0.02
    vlm_clip: float = 0.5

    # è·é›¢é‡Œç¨‹ç¢‘ï¼ˆåˆ†æ®µçå‹µï¼‰
    distance_bins: Tuple[float, ...] = (0.05, 0.04, 0.03, 0.02, 0.01)
    distance_vals: Tuple[float, ...] = (0.05, 0.15, 0.30, 0.60, 1.00)


# =====================================
# ğŸ–¼ï¸ å½±åƒè™•ç†
# =====================================
def preprocess_image(image_rgb: np.ndarray, size: Tuple[int, int]) -> np.ndarray:
    """
    å°‡ç›¸æ©Ÿå½±åƒç¸®æ”¾ç‚ºæŒ‡å®šå°ºå¯¸ã€‚
    :param image_rgb: HxWx3 RGB uint8
    :param size: (W, H)
    :return: ç¸®æ”¾å¾Œå½±åƒ HxWx3 uint8
    """
    if image_rgb is None:
        raise ValueError("image_rgb is None")
    if image_rgb.dtype != np.uint8:
        image_rgb = np.clip(image_rgb, 0, 255).astype(np.uint8)
    if image_rgb.ndim != 3 or image_rgb.shape[2] != 3:
        raise ValueError("image_rgb å¿…é ˆæ˜¯ HxWx3")

    w, h = size
    return cv2.resize(image_rgb, (w, h), interpolation=cv2.INTER_AREA)


# =====================================
# ğŸ” å‹•ä½œæ­·å²è¨˜éŒ„
# =====================================
class ActionHistory:
    """
    å„²å­˜æœ€è¿‘ N æ¬¡å‹•ä½œï¼Œæä¾›å±•å¹³å‘é‡è¼¸å‡ºã€‚
    """
    def __init__(self, max_len: int, action_dim: Optional[int] = None):
        self.max_len = int(max_len)
        self._dq: Deque[np.ndarray] = deque(maxlen=self.max_len)
        self._action_dim: Optional[int] = action_dim

    def clear(self) -> None:
        self._dq.clear()

    def add(self, action: np.ndarray | Sequence[float]) -> None:
        a = np.asarray(action, dtype=np.float32)
        if self._action_dim is None:
            self._action_dim = int(a.size)
        elif a.size != self._action_dim:
            raise ValueError(f"action size {a.size} != expected {self._action_dim}")
        self._dq.append(a)

    def vector(self) -> np.ndarray:
        if self._action_dim is None:
            return np.zeros(0, dtype=np.float32)
        out = np.zeros((self.max_len, self._action_dim), dtype=np.float32)
        k = min(len(self._dq), self.max_len)
        if k > 0:
            seq = np.stack(list(self._dq)[-k:], axis=0)
            out[-k:] = seq
        return out.reshape(-1)


# =====================================
# âš ï¸ å‹•ä½œæ‡²ç½°ï¼ˆæ­£å‰‡åŒ–ï¼‰
# =====================================
def compute_action_cost(action: np.ndarray | Sequence[float], coef: float) -> float:
    """
    L2 å‹•ä½œæ‡²ç½°ï¼Œé˜²æ­¢éœ‡ç›ªã€‚
    """
    a = np.asarray(action, dtype=np.float32)
    return float(coef * np.linalg.norm(a))


# =====================================
# ğŸŒŸ VLM ç¯€æµæ§åˆ¶ï¼ˆå¤šæ­¥åŠ åˆ†ï¼‰
# =====================================
class VLMThrottle:
    """
    æ§åˆ¶æ˜¯å¦è§¸ç™¼ VLM æ‰“åˆ†ï¼ˆæ¸›å°‘é »ç‡ï¼‰
    - reset(init_score) é‡è¨­åˆå§‹åˆ†æ•¸
    - maybe_bonus(...) æ»¿è¶³æ¢ä»¶æ‰å‘¼å« VLM è©•åˆ†
    """
    def __init__(self, interval: int, deltaE_thresh: float, clip: float):
        self.interval = int(interval)
        self.deltaE_thresh = float(deltaE_thresh)
        self.clip = float(clip)
        self.steps = 0
        self.last_score = 0.0

    def reset(self, init_score: float = 0.0) -> None:
        self.steps = 0
        self.last_score = float(init_score)

    def maybe_bonus(self, delta_E: float, image_rgb: np.ndarray, vlm) -> float:
        self.steps += 1
        if not (delta_E > self.deltaE_thresh and self.steps >= self.interval):
            return 0.0
        s = float(vlm.score_image(image_rgb, instruction="align objects in a row"))
        dv = float(np.clip(s - self.last_score, -self.clip, self.clip))
        self.last_score = s
        self.steps = 0
        return dv


# =====================================
# ğŸ§  é‡Œç¨‹ç¢‘å·¥å…·ï¼ˆè·é›¢/æ¥è§¸ï¼‰
# =====================================

def validate_milestones(bins: Sequence[float], vals: Sequence[float]):
    """å°‡è·é›¢é–€æª»èˆ‡çå‹µæ’åºç‚ºã€è·é›¢ç”±å°åˆ°å¤§ã€çå‹µä¸éæ¸›ã€ä¸¦æª¢æŸ¥æœ‰æ•ˆæ€§ã€‚"""
    b = np.asarray(bins, dtype=float)
    v = np.asarray(vals, dtype=float)
    if b.size != v.size:
        raise ValueError("distance_bins èˆ‡ distance_vals é•·åº¦å¿…é ˆç›¸åŒ")
    order = np.argsort(b)  # è·é›¢å°â†’å¤§
    b, v = b[order], v[order]
    if not np.all(np.diff(v) >= 0):
        raise ValueError("distance_vals å¿…é ˆå°æ‡‰ç‚ºééæ¸›ï¼ˆè¶Šè¿‘çå‹µä¸æœƒè®Šå°ï¼‰")
    return b, v


class DistanceMilestone:
    """
    åˆ†æ®µè·é›¢çå‹µï¼ˆæ¯æ®µåªçµ¦ä¸€æ¬¡ï¼‰ã€‚
    - reset()ï¼šæ¸…é™¤å·²é ˜å–çš„æ®µä½
    - update(d)ï¼šè‹¥é¦–æ¬¡é”æˆæ›´åš´æ ¼çš„è·é›¢æ®µä½ï¼Œå›å‚³è©²æ®µä½çå‹µï¼Œå¦å‰‡ 0
    æ³¨æ„ï¼šbins/vals æœƒè‡ªå‹•æŒ‰è·é›¢ç”±å°åˆ°å¤§æ’åºï¼ˆ0.01, 0.02, ...ï¼‰
    """
    def __init__(self, bins: Sequence[float], vals: Sequence[float]):
        self.bins, self.vals = validate_milestones(bins, vals)
        self._claimed = np.zeros_like(self.bins, dtype=bool)

    def reset(self):
        self._claimed[:] = False

    def update(self, d: float) -> float:
        # å¾æœ€åš´æ ¼ï¼ˆæœ€å°è·é›¢ï¼‰å¾€å¤–æª¢æŸ¥ï¼Œå‘½ä¸­ç¬¬ä¸€å€‹æœªé ˜å–é–€æª»å°±çµ¦è©²æ®µä½çå‹µ
        for i in range(len(self.bins)):
            if d < self.bins[i] and not self._claimed[i]:
                self._claimed[i] = True
                return float(self.vals[i])
        return 0.0


class ContactOnceBonus:
    """
    æ¥è§¸ä¸€æ¬¡æ€§åŠ åˆ†ï¼šåŒä¸€ç‰©ä»¶åœ¨å–®ä¸€ episode åªåŠ åˆ†ä¸€æ¬¡ã€‚
    å¯é¸æ“‡åœ¨åŠ åˆ†åŒæ™‚è§¸ç™¼ side-effectï¼ˆä¾‹å¦‚ attach ç‰©ä»¶ï¼‰ã€‚
    """
    def __init__(self, bonus: float, on_first_contact=None):
        self.bonus = float(bonus)
        self.on_first_contact = on_first_contact  # å¯å‚³å…¥ callable(name: str)
        self._seen: set[str] = set()

    def reset(self):
        self._seen.clear()

    def award_if_first(self, name: str) -> float:
        if name in self._seen:
            return 0.0
        self._seen.add(name)
        if callable(self.on_first_contact):
            try:
                self.on_first_contact(name)
            except Exception:
                pass
        return self.bonus


# =====================================
# ğŸªœ è·é›¢åˆ†æ®µçå‹µ
# =====================================
def distance_milestone_reward(d: float, bins: Sequence[float], vals: Sequence[float]) -> float:
    """å›å‚³ã€ç›®å‰è·é›¢æ‰€èƒ½é”åˆ°çš„æœ€é«˜æ®µä½çå‹µã€ï¼›è‹¥æœªå‘½ä¸­ä»»ä½•æ®µä½å‰‡å›å‚³ 0ã€‚"""
    b, v = validate_milestones(bins, vals)
    # æ‰¾åˆ°æœ€åš´æ ¼ï¼ˆæœ€å°é–€æª»ï¼‰ä½†ä»æ»¿è¶³ d < b[i] çš„æ®µä½ï¼›ä»¥é«˜æ®µä½ç‚ºå„ªå…ˆ
    for i in range(len(b)):
        if d < b[i]:
            return float(v[i])
    return 0.0


def distance_sparse_bonus(d: float, bins: Sequence[float], vals: Sequence[float]) -> float:
    """
    [Deprecated]ï¼šè«‹æ”¹ç”¨ `distance_milestone_reward()` æˆ–ç‹€æ…‹åŒ–çš„ `DistanceMilestone`ã€‚
    ç‚ºå‘å¾Œç›¸å®¹ï¼Œé€™è£¡å›å‚³èˆ‡ `distance_milestone_reward` ç›¸åŒé‚è¼¯ä¹‹çµæœã€‚
    """
    return distance_milestone_reward(d, bins, vals)


# =====================================
# ğŸ§° è–„ä»‹é¢ï¼šè®“å¤–éƒ¨ç¨‹å¼æ›´å¥½ç”¨ï¼ˆFunctional Facadeï¼‰
# =====================================
from typing import Callable, NamedTuple

class RewardHelpers(NamedTuple):
    """å›å‚³çµ¦ä½¿ç”¨è€…çš„ä¸€çµ„ã€ä¸€æ­¥åˆ°ä½ã€çš„å‡½å¼èˆ‡ç‹€æ…‹å®¹å™¨ã€‚"""
    reset: Callable[[], None]
    distance_bonus: Callable[[float], float]
    contact_bonus: Callable[[str], float]
    vlm_bonus: Callable[[float, np.ndarray, object], float]
    action_cost: Callable[[np.ndarray], float]
    hist: ActionHistory
    hist_vec: Callable[[], np.ndarray]


def new_episode_helpers(cfg: EnvConfig, attach: Optional[Callable[[str], None]] = None) -> RewardHelpers:
    """
    å»ºç«‹å³é–‹å³ç”¨çš„è¼”åŠ©å™¨ï¼š
    - è·é›¢åˆ†æ®µçå‹µï¼ˆæ¯æ®µåªç™¼ä¸€æ¬¡ï¼‰
    - å¤¾çˆªé›™æŒ‡æ¥è§¸ä¸€æ¬¡æ€§çå‹µï¼ˆåŒç‰©ä»¶å–®å›åˆåªç™¼ä¸€æ¬¡ï¼Œå¯é¸æ“‡åŒæ­¥ attachï¼‰
    - VLM ç¯€æµåŠ åˆ†ï¼ˆéœ€å‚³å…¥ Î”E èˆ‡ç•¶å‰ç•«é¢ï¼‰
    - å‹•ä½œæˆæœ¬ï¼ˆL2ï¼‰
    - å‹•ä½œæ­·å²ï¼ˆå¯ç›´æ¥å– flattened å‘é‡ï¼‰

    ç”¨æ³•ï¼š
        helpers = new_episode_helpers(cfg, attach_fn)
        helpers.reset()
        r += helpers.distance_bonus(d)
        r += helpers.contact_bonus(target_name)
        r += helpers.vlm_bonus(delta_E, img_rgb, vlm)
        r -= helpers.action_cost(action)
        obs_hist = helpers.hist_vec()
    """
    _dist = DistanceMilestone(cfg.distance_bins, cfg.distance_vals)
    _contact = ContactOnceBonus(cfg.contact_bonus, on_first_contact=attach)
    _vlm = VLMThrottle(cfg.vlm_interval, cfg.vlm_deltaE_thresh, cfg.vlm_clip)
    _hist = ActionHistory(cfg.action_hist_len)

    def _reset():
        _dist.reset()
        _contact.reset()
        _vlm.reset(0.0)
        _hist.clear()

    def _distance_bonus(d: float) -> float:
        return _dist.update(float(d))

    def _contact_bonus(name: str) -> float:
        return _contact.award_if_first(str(name))

    def _vlm_bonus(delta_E: float, image_rgb: np.ndarray, vlm) -> float:
        return _vlm.maybe_bonus(float(delta_E), image_rgb, vlm)

    def _action_cost(a: np.ndarray) -> float:
        return compute_action_cost(a, cfg.action_cost_coef)

    return RewardHelpers(
        reset=_reset,
        distance_bonus=_distance_bonus,
        contact_bonus=_contact_bonus,
        vlm_bonus=_vlm_bonus,
        action_cost=_action_cost,
        hist=_hist,
        hist_vec=_hist.vector,
    )


# --- ç„¡ç‹€æ…‹ç‰ˆï¼ˆä½ åªæƒ³ä¸€è¡Œç®—ç•¶ä¸‹å¯æ‹¿åˆ°çš„è·é›¢çå‹µï¼‰ ---

def distance_bonus_now(d: float, bins: Sequence[float], vals: Sequence[float]) -> float:
    return distance_milestone_reward(d, bins, vals)


def contact_bonus_once(seen: set, name: str, bonus: float, attach: Optional[Callable[[str], None]] = None) -> float:
    """ç„¡é¡åˆ¥ç‰ˆæœ¬ï¼šç”¨ä¸€å€‹ set è¨˜éŒ„æœ¬å›åˆç¢°éçš„ç‰©ä»¶ã€‚"""
    if name in seen:
        return 0.0
    seen.add(name)
    if callable(attach):
        try:
            attach(name)
        except Exception:
            pass
    return float(bonus)
